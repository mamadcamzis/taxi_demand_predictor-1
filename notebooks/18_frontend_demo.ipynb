{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mcamara/taxi_demand_predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting libraries\n",
    "import streamlit as st\n",
    "import geopandas as gpd\n",
    "import pydeck as pdk\n",
    "\n",
    "from src.inference import load_predictions_from_store\n",
    "from src.paths import DATA_DIR\n",
    "from src.plot import plot_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-03-29 09:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = pd.to_datetime(datetime.utcnow()).floor('H')\n",
    "# current_date = pd.Timestamp('2023-03-01 11:00:00')\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch file with shape data\n",
    "from geopandas.geodataframe import GeoDataFrame\n",
    "\n",
    "def load_shape_data_file() -> GeoDataFrame:\n",
    "    \"\"\"Fetches remote file with shape data, that we later use to plot the\n",
    "    different pickup_location_ids on the map of NYC.\n",
    "\n",
    "    Raises:\n",
    "        Exception: raised when we cannot connect to the external server where\n",
    "        the file is.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: columns -> (OBJECTID\tShape_Leng\tShape_Area\tzone\tLocationID\tborough\tgeometry)\n",
    "    \"\"\"\n",
    "    # download file\n",
    "    URL = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip'\n",
    "    response = requests.get(URL)\n",
    "    path = DATA_DIR / f'taxi_zones.zip'\n",
    "    if response.status_code == 200:\n",
    "        open(path, \"wb\").write(response.content)\n",
    "    else:\n",
    "        raise Exception(f'{URL} is not available')\n",
    "\n",
    "    # unzip file\n",
    "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR / 'taxi_zones')\n",
    "\n",
    "    # load and return shape file\n",
    "    return gpd.read_file(DATA_DIR / 'taxi_zones/taxi_zones.shp').to_crs('epsg:4326')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = load_shape_data_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/24729\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature view model_predictions_feature_view               already existed. Skipped creation.\n",
      "Fetching predictions for `pickup_hours` between 2023-03-29 08:00:00  and 2023-03-29 09:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No training dataset version was provided to initialise batch scoring . Defaulting to version 1.\n",
      "2023-03-29 11:24:57.648 INFO    pyhive.hive: USE `taxi_demand_mc_featurestore`\n",
      "UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "2023-03-29 11:24:58.365 INFO    pyhive.hive: SELECT `fg0`.`pickup_hour` `pickup_hour`, `fg0`.`rides` `rides`, `fg0`.`pickup_location_id` `pickup_location_id`\n",
      "FROM `taxi_demand_mc_featurestore`.`time_series_hourly_feature_group_1` `fg0`\n",
      "WHERE `fg0`.`pickup_hour` >= TIMESTAMP '2023-03-28 08:00:00.000' AND `fg0`.`pickup_hour` <= TIMESTAMP '2023-03-30 09:00:00.000'\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/24729/featurestores/24649/featureview/model_predictions_feature_view/version/1/trainingdatasets/version/1). Server response: \nHTTP code: 404, HTTP reason: Not Found, error code: 270012, error msg: Training dataset wasn't found., user msg: FeatureView name: model_predictions_feature_view, version: 1, td version: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m timedelta\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference\u001b[39;00m \u001b[39mimport\u001b[39;00m load_predictions_from_store\n\u001b[0;32m----> 4\u001b[0m predictions_df \u001b[39m=\u001b[39m load_predictions_from_store(\n\u001b[1;32m      5\u001b[0m     from_pickup_hour\u001b[39m=\u001b[39;49mcurrent_date \u001b[39m-\u001b[39;49m timedelta(hours\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m      6\u001b[0m     to_pickup_hour\u001b[39m=\u001b[39;49mcurrent_date\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/taxi_demand_predictor/src/inference.py:162\u001b[0m, in \u001b[0;36mload_predictions_from_store\u001b[0;34m(from_pickup_hour, to_pickup_hour)\u001b[0m\n\u001b[1;32m    156\u001b[0m predictions_fv \u001b[39m=\u001b[39m feature_store\u001b[39m.\u001b[39mget_feature_view(\n\u001b[1;32m    157\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mFEATURE_VIEW_MODEL_PREDICTIONS,\n\u001b[1;32m    158\u001b[0m     version\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFetching predictions for `pickup_hours` between \u001b[39m\u001b[39m{\u001b[39;00mfrom_pickup_hour\u001b[39m}\u001b[39;00m\u001b[39m  and \u001b[39m\u001b[39m{\u001b[39;00mto_pickup_hour\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m predictions \u001b[39m=\u001b[39m predictions_fv\u001b[39m.\u001b[39;49mget_batch_data(\n\u001b[1;32m    163\u001b[0m     start_time\u001b[39m=\u001b[39;49mfrom_pickup_hour \u001b[39m-\u001b[39;49m timedelta(days\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    164\u001b[0m     end_time\u001b[39m=\u001b[39;49mto_pickup_hour \u001b[39m+\u001b[39;49m timedelta(days\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    165\u001b[0m )\n\u001b[1;32m    166\u001b[0m predictions \u001b[39m=\u001b[39m predictions[predictions\u001b[39m.\u001b[39mpickup_hour\u001b[39m.\u001b[39mbetween(\n\u001b[1;32m    167\u001b[0m     from_pickup_hour, to_pickup_hour)]\n\u001b[1;32m    169\u001b[0m \u001b[39m# sort by `pick_up_hour` and `pickup_location_id`\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-KlxhZ9VN-py3.9/lib/python3.9/site-packages/hsfs/feature_view.py:301\u001b[0m, in \u001b[0;36mFeatureView.get_batch_data\u001b[0;34m(self, start_time, end_time, read_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_scoring_server \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_batch_scoring()\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_view_engine\u001b[39m.\u001b[39;49mget_batch_data(\n\u001b[1;32m    302\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    303\u001b[0m     start_time,\n\u001b[1;32m    304\u001b[0m     end_time,\n\u001b[1;32m    305\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_scoring_server\u001b[39m.\u001b[39;49mtraining_dataset_version,\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_scoring_server\u001b[39m.\u001b[39;49m_transformation_functions,\n\u001b[1;32m    307\u001b[0m     read_options,\n\u001b[1;32m    308\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-KlxhZ9VN-py3.9/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py:401\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_data\u001b[0;34m(self, feature_view_obj, start_time, end_time, training_dataset_version, transformation_functions, read_options)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_group_accessibility(feature_view_obj)\n\u001b[1;32m    397\u001b[0m feature_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_batch_query(\n\u001b[1;32m    398\u001b[0m     feature_view_obj, start_time, end_time, with_label\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    399\u001b[0m )\u001b[39m.\u001b[39mread(read_options\u001b[39m=\u001b[39mread_options)\n\u001b[0;32m--> 401\u001b[0m training_dataset_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_training_data_metadata(\n\u001b[1;32m    402\u001b[0m     feature_view_obj, training_dataset_version\n\u001b[1;32m    403\u001b[0m )\n\u001b[1;32m    404\u001b[0m training_dataset_obj\u001b[39m.\u001b[39mtransformation_functions \u001b[39m=\u001b[39m transformation_functions\n\u001b[1;32m    406\u001b[0m \u001b[39mreturn\u001b[39;00m engine\u001b[39m.\u001b[39mget_instance()\u001b[39m.\u001b[39m_apply_transformation_function(\n\u001b[1;32m    407\u001b[0m     training_dataset_obj, dataset\u001b[39m=\u001b[39mfeature_dataframe\n\u001b[1;32m    408\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-KlxhZ9VN-py3.9/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py:348\u001b[0m, in \u001b[0;36mFeatureViewEngine._get_training_data_metadata\u001b[0;34m(self, feature_view_obj, training_dataset_version)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_training_data_metadata\u001b[39m(\u001b[39mself\u001b[39m, feature_view_obj, training_dataset_version):\n\u001b[0;32m--> 348\u001b[0m     td \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_view_api\u001b[39m.\u001b[39;49mget_training_dataset_by_version(\n\u001b[1;32m    349\u001b[0m         feature_view_obj\u001b[39m.\u001b[39;49mname, feature_view_obj\u001b[39m.\u001b[39;49mversion, training_dataset_version\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    351\u001b[0m     \u001b[39m# schema and transformation functions need to be set for writing training data or feature serving\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     td\u001b[39m.\u001b[39mschema \u001b[39m=\u001b[39m feature_view_obj\u001b[39m.\u001b[39mschema\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-KlxhZ9VN-py3.9/lib/python3.9/site-packages/hsfs/core/feature_view_api.py:168\u001b[0m, in \u001b[0;36mFeatureViewApi.get_training_dataset_by_version\u001b[0;34m(self, name, version, training_dataset_version)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_training_dataset_by_version\u001b[39m(\u001b[39mself\u001b[39m, name, version, training_dataset_version):\n\u001b[1;32m    166\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_training_data_base_path(name, version, training_dataset_version)\n\u001b[1;32m    167\u001b[0m     \u001b[39mreturn\u001b[39;00m training_dataset\u001b[39m.\u001b[39mTrainingDataset\u001b[39m.\u001b[39mfrom_response_json_single(\n\u001b[0;32m--> 168\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_send_request(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, path)\n\u001b[1;32m    169\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-KlxhZ9VN-py3.9/lib/python3.9/site-packages/hsfs/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inst\u001b[39m.\u001b[39m_connected:\n\u001b[1;32m     34\u001b[0m     \u001b[39mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[39mreturn\u001b[39;00m fn(inst, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-KlxhZ9VN-py3.9/lib/python3.9/site-packages/hsfs/client/base.py:171\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    168\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\u001b[39m.\u001b[39msend(prepped, verify\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify, stream\u001b[39m=\u001b[39mstream)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/24729/featurestores/24649/featureview/model_predictions_feature_view/version/1/trainingdatasets/version/1). Server response: \nHTTP code: 404, HTTP reason: Not Found, error code: 270012, error msg: Training dataset wasn't found., user msg: FeatureView name: model_predictions_feature_view, version: 1, td version: 1"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from src.inference import load_predictions_from_store\n",
    "\n",
    "predictions_df = load_predictions_from_store(\n",
    "    from_pickup_hour=current_date - timedelta(hours=1),\n",
    "    to_pickup_hour=current_date\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-B6bWjkJU-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
